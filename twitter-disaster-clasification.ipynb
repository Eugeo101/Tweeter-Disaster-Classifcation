{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":380554,"sourceType":"datasetVersion","datasetId":167266},{"sourceId":1497,"sourceType":"modelInstanceVersion","modelInstanceId":1265,"modelId":191},{"sourceId":2938,"sourceType":"modelInstanceVersion","modelInstanceId":2180,"modelId":244},{"sourceId":2937,"sourceType":"modelInstanceVersion","modelInstanceId":2180,"modelId":244},{"sourceId":2622,"sourceType":"modelInstanceVersion","modelInstanceId":1899,"modelId":244}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Manpulate\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n# Feature Extraction\n# import user_agents # get info from user_agent (browser_info)\n# from ip2geotools.databases.noncommercial import DbIpCity as ip2geo # get location from ip\n# from geopy.distance import great_circle # distance btn 2 (lat,long)\n# from geopy.geocoders import Nominatim # geocode(\"place\") / reverse(\"lat,long\")\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # text feature\n\n# Pre-Processing\nfrom sklearn.model_selection import train_test_split # train-test-split\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer # detect & handle NaNs\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder # Ordinal Encoding, Nominal Encoding\nfrom category_encoders import BinaryEncoder # Nominal Encoding \nfrom imblearn.under_sampling import RandomUnderSampler # undersampling\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE # oversampling\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler # Scaling\n\n# Modeling\n## 1) Pipeline\nfrom sklearn.pipeline import Pipeline, make_pipeline # to make pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector # apply pipeline to each column\n\n## 2) Regression Models\nfrom sklearn.linear_model import LinearRegression # if data is small and small_no_features\nfrom sklearn.linear_model import SGDRegressor # if data is large: (can have penalty=constrains)\nfrom sklearn.preprocessing import PolynomialFeatures # for polynomial regresion (then apply scaling after it)\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV # Regularization \n\n## 2') Classfication Models\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB, MultinomialNB\nfrom sklearn.svm import LinearSVC, SVC, LinearSVR, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import VotingClassifier, VotingRegressor # Ensemble (Voting)\nfrom sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor # Bagging & Pasting\nfrom sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingClassifier, HistGradientBoostingRegressor # Boosting\nfrom sklearn.ensemble import StackingClassifier, StackingRegressor # Stacking\n\n## 3) Model Selection (Underfitting vs Overfitting) [bias variance tradeoff => perfect model complexity]\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, GridSearchCV, RandomizedSearchCV # (Train - Valid - Test) + hyperparameters tunning \nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV # if data / features is large\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error # Evaluate Model: r2=> accuracy, L2-norm: if no outliers, L1-norm: if outliers\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, roc_auc_score\nfrom scipy import stats # Confidence Interval of Accuracy / Loss / Utility\nimport joblib # save model\n\n# 4) Dimensionality reduction\nfrom sklearn.decomposition import PCA, IncrementalPCA # till 20K features\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection # >20k features\nfrom sklearn.manifold import LocallyLinearEmbedding, MDS, Isomap, TSNE # Manifold could be better than Projection\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis # for classfication problems (larg distance between diffrent classes)\n\n# 5) Clustering\nfrom sklearn.cluster import KMeans, MiniBatchKMeans # spherical dataset (n_cluster by (elbow / silhouette_score / silhoutette_samples)) \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.cluster import DBSCAN # eps by K-distanceGraph\nfrom sklearn.neighbors import NearestNeighbors # determine nearest neighbor\n# from sklearn.cluster import HDBSCAN # state of art\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\n# 6) Anomaly Detection\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\nfrom sklearn.covariance import MinCovDet\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# 7) Time Series Models\n# Naive using df.diff(s)\nfrom statsmodels.tsa.arima.model import ARIMA # Statstical Models as ARIMA and VAR\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.api import VAR # Handle Seasonality and Trends\nfrom xgboost import XGBRegressor, XGBClassifier # Machine Learning Model: extract lag features\nfrom prophet import Prophet # fb prophet: trending model for forcasting\n# from pandas.tseries.holiday import USFederalHolidayCalendar as Calender\n# cal = Calender() # all holidays\n# holidays = cal.holidays(start=df.index.min(), end=df.index.max(), return_name=True)\n# holidays_df = pd.DataFrame(data=holidays, columns=['holiday']).reset_index().rename(columns={\"index\":'ds'})\n# holidays_df\n# holidays_df.groupby('holiday')[['ds']].count()\n\n# 8) Recommendation System\n# basic concept [collaborative, content, hybird]\n## correlation + nearest_neighbor with cosine_similarity\nfrom sklearn.neighbors import NearestNeighbors # content / collaborative\nfrom sklearn.metrics.pairwise import sigmoid_kernel, rbf_kernel, cosine_similarity, cosine_distances # content / collaborative\nfrom sklearn.decomposition import NMF, TruncatedSVD # Collaborative using ML\n\n# Deep Learning: Strongest\nimport tensorflow as tf\n## model\nfrom tensorflow.keras.models import Sequential # Sequntial API\nfrom tensorflow.keras.models import Model # Functional API & SubclassAPI\nfrom tensorflow.keras.layers import * # layers to put\n## Compile\nfrom tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, Huber # regression loss\nfrom tensorflow.keras.optimizers import SGD # (SGD, Momentum, NAG) not adaptive optimizers (tune lr)\nfrom tensorflow.keras.optimizers import RMSprop, Adam, Adamax, Nadam, AdamW, Adagrad # adaptive (no need to tune lr)\n# lr schedulers\n# 1) Power Scheduling # Adam(learning_rate=0.001, decay=1e-4) # 1/s = decay\n# others in optimizers.ipynb using LeearningRateScheduler\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n# for performance schedule\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n# lr_schedule on steps\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay, PiecewiseConstantDecay \nfrom tensorflow.keras.metrics import RootMeanSquaredError # metrics\n\n## Callbacks\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\nfrom pathlib import Path # for tensorboard\nfrom time import strftime # for tensorboard\n\n# Regularization: EarlyStoping, AuxOutput, L1_l2, MCDropout (Dropout(rate=0.2, training=True))\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2 # Dropout at layers\nfrom tensorflow.keras.constraints import max_norm\n\n\n# tf.data\nimport glob # read multiple files\n# tf.data.Dataset.list_files # which can read multiple files => then interleave btn them / map / shuffle / batch then prefetch at last\n# import tensorflow_datasets as tfds # or use it to load datasets from 'https://www.tensorflow.org/datasets/' as datasets.batch(32) and so on\n\n# Keras Preprocessing\nfrom tensorflow.keras.layers import Normalization, Discretization, BatchNormalization # numeric\nfrom tensorflow.keras.layers import CategoryEncoding, Hashing, StringLookup, TextVectorization, Embedding # NLP: text or load pre-trained model from tensorflow_hub\nfrom tensorflow.keras.layers import Rescaling, Resizing, CenterCrop # for images\n\n# Pre Trained Models also of keras\n# https://keras.io/api/applications/\nfrom tensorflow.keras.applications.resnet50 import ResNet50 # model \nfrom tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions # preprocessing & decode output\n\n# Pre Trained Models\nimport tensorflow_hub as hub\n\n# Fine Tune NN (GridSearch)\nimport keras_tuner as kt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1) Load Data\n\n* 1) Understand Columns\n* 2) Check dtypes\n* 3) describe numeric\n* 4) describe categorical","metadata":{}},{"cell_type":"code","source":"# 1) Understand Columns\ndf = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2) Check dtypes\ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3) describe numeric\ndf.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols = df.select_dtypes(include='O').columns\nfor col in cat_cols:\n    print(f\"Number of uniques in \\'{col}\\' is: {df[col].nunique()}\")\n    print(f\"Uniques in \\'{col}\\' is:\\n{df[col].unique()}\")\n    print()\n    print('*' * 50)\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2) EDA","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_50_words = df['keyword'].value_counts().index[:30]\ndff_top50_target_count = df[df['keyword'].isin(top_50_words)].groupby('keyword')['target'].value_counts().unstack()\ndff_top50_target_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# as there is NaNs replace it with 0 (which means not found)\ndff_top50_target_count.fillna(0, inplace=True)\ndff_top50_target_count.reset_index(inplace=True)\ndff_top50_target_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filter_50 = df['keyword'].isin(dff_top50_target_count['keyword'])\ndff = df[filter_50]\ndff","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# this figure means that keyword is not effective and can be missleading as a feature to the model\n# so we should rely on context/meaning of text\nplt.figure(figsize=(12, 12))\nsns.barplot(y=dff['keyword'], x=dff['id'], hue=dff['target'], estimator=len)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['keyword'] == 'weapon']['text'].iloc[0] # Disaster","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['keyword'] == 'weapon']['text'].iloc[1] # Not Disaster\n# so it depends on context not keyword only 'weapon'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# however some keywords only appear as 0 or only appear as 1 so it's interesting to include keyword\n# as a feature. as it appeared multiple times as 0 or 1\n\ndf['target_mean'] = df.groupby('keyword')['target'].transform('mean')\n# to propgate mean to each keyword (like window function)\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=df.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndf.drop(columns=['target_mean'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# let's dive more on how the text is written in both classes\ntexts = df[df['keyword'] == 'weapon'][['text', 'target']].iloc[0:30].to_numpy()\nfor text in texts:\n    print(text[0], text[1])\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### insights:\n##### in these 10 text rows\n##### most of disaster tweets are writen in long text formal way (bcz they are written by news agencies)\n##### while most of non disaster tweets are writen in unformal and short length (bcz they are written by individuals)","metadata":{}},{"cell_type":"code","source":"# the same way it won't depend on location (which have 2500 NaNs already) but the context also\ndf[df['keyword'] == 'weapon']['location'].iloc[0], df[df['keyword'] == 'weapon']['location'].iloc[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3) Preprocessing\n\n* 1) Extract text features\n\n* 2) check embedding coverage (then clean text and recheck)\n \n* 3) check duplicates after cleaning  ","metadata":{}},{"cell_type":"markdown","source":"#### 1.Extract text features\n\n##### Meta Features\n\nDistributions of meta features in classes and datasets can be helpful to identify disaster tweets. It looks like disaster tweets are written in a more formal way with longer words compared to non-disaster tweets because most of them are coming from news agencies. Non-disaster tweets have more typos than disaster tweets because they are coming from individual users. The meta features used for the analysis are;\n\n* `word_count` number of words in text\n* `unique_word_count` number of unique words in text\n* `stop_word_count` number of stop words in text\n* `punctuation_count` number of punctuations in text\n* `char_count` number of characters in text [to be tested]\n* `mean_word_length` average character count in words [to be tested]\n* `url_count` number of urls in text [to be tested]\n* `mention_count` number of mentions (@) in text [to be tested]\n* `hashtag_count` number of hashtags (#) in text [to be tested]","metadata":{}},{"cell_type":"code","source":"# Get English stopwords\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words('english'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# punctuation\nimport string\nstring.punctuation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = df.copy()\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# word_count\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\ndf_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\ndf_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n\n# url_count\ndf_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ndf_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndf_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndf_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ndf_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndf_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ndf_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 2. check embedding coverage (then clean text and recheck)","metadata":{}},{"cell_type":"code","source":"# check embedding coverage\nglove_embeddings = np.load(\"/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\", allow_pickle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import operator\n\ndef build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) / len(vocab)\n    text_coverage = (n_covered / (n_covered + n_oov))\n    \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage\n\ntrain_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['text'], glove_embeddings)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['text'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clean text and recheck\ntrain_glove_oov[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"common_words = train_glove_oov.copy()\nsorted(common_words, key=lambda x: x[1])\ncommon_words[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### So this text really require cleaning before continue let us make clean\n\n#### Text Cleaning\nTweets require lots of cleaning but it is inefficient to clean every single tweet because that would consume too much time. A general approach must be implemented for cleaning.\n\n* The most common type of words that require cleaning in oov have punctuations at the start or end. Those words doesn't have embeddings because of the trailing punctuations. Punctuations #, @, !, ?, +, &, -, $, =, <, >, |, {, }, ^, ', (, ),[, ], *, %, ..., ', ., :, ; are separated from words\n* Special characters that are attached to words are removed completely\n* Contractions are expanded\n* Urls are removed\n* Character entity references are replaced with their actual symbols\n* Typos and slang are corrected, and informal abbreviations are written in their long forms\n* Some words are replaced with their acronyms and some words are grouped into one\n* Finally, hashtags and usernames contain lots of information about the context but they are written without spaces in between words so they don't have embeddings. Informational usernames and hashtags should be expanded but there are too many of them. I expanded as many as I could, but it takes too much time to run clean function after adding those replace calls.","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport re\n\ndef clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n           \n    # Urls\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n        \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')      \n        \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    return tweet\n\ndf_train['text_cleaned'] = df_train['text'].apply(lambda s : clean(s))\ndf_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))\n\ntrain_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['text_cleaned'], glove_embeddings)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['text_cleaned'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Mislabeled Samples (duplicates)\ndf_train.duplicated().sum() # no duplicate rows","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# however let's check if there is duplicate text have 2 diff targets\n\ndf_mislabeled = df_train.groupby(['text'])[['target']].nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target'] # sentences have 2 targets (so ppl confused by them)\ndf_mislabeled.index.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['target_relabeled'] = df_train['target'].copy() \n\ndf_train.loc[df_train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\ndf_train.loc[df_train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\ndf_train.loc[df_train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\ndf_train.loc[df_train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\ndf_train.loc[df_train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\ndf_train.loc[df_train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\ndf_train.loc[df_train['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import operator\n\ndef build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) / len(vocab)\n    text_coverage = (n_covered / (n_covered + n_oov))\n    \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage, vocab\n\ntrain_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage, train_vocab = check_embeddings_coverage(df_train['text_cleaned'], glove_embeddings)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage, _ = check_embeddings_coverage(df_test['text_cleaned'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_vocab) # we have vocabulary size of 21K words","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4) Modeling\n\n* 1) build simple sentiment analysis model on cleaned text and target\n* 2) bias vs variance (if model underfit incerase features)\n* 3) test performance\n* 4) save model ","metadata":{"execution":{"iopub.status.busy":"2025-03-03T19:30:19.268759Z","iopub.execute_input":"2025-03-03T19:30:19.268959Z","iopub.status.idle":"2025-03-03T19:30:19.290856Z","shell.execute_reply.started":"2025-03-03T19:30:19.268941Z","shell.execute_reply":"2025-03-03T19:30:19.289628Z"}}},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['target_relabeled'] = df_train['target_relabeled'].astype(np.int16)\ndf_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) build simple sentiment analysis model on cleaned text and target\ndef create_dataset(dataframe, shuffle=False):\n    text_data = dataframe[\"text_cleaned\"].astype(str).to_numpy()\n    target_data = dataframe[\"target_relabeled\"].astype(np.int32).to_numpy()  # Ensure int32 type\n    \n    ds = tf.data.Dataset.from_tensor_slices((text_data, target_data))\n    \n    # ds = ds.map(lambda row: (row[0], row[1])) # X, y\n    # preprocessing done by regular expression\n    if shuffle:\n        ds = ds.shuffle(10_000, seed=42)\n    return ds.batch(32).prefetch(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_size = int(df_train.shape[0] * 0.8) # 80% train\n# valid_size = int(df_train.shape[0] * 0.9) # 10% valid - 10% test\n\n\n# train_df = create_dataset(df_train.iloc[:train_size], shuffle=True)\n# valid_df = create_dataset(df_train.iloc[train_size:valid_size])\n# test_df = create_dataset(df_train.iloc[valid_size:])\n\nfrom sklearn.model_selection import train_test_split\ntrain_ratio = 0.8  # 80% for training\nvalid_ratio = 0.1  # 10% for validation\ntest_ratio = 0.1   # 10% for testing\n\n\n# Train Valid and Test\ntrain_valid_df, testt_df = train_test_split(df_train, test_size=test_ratio, random_state=42, stratify=df_train[\"target_relabeled\"])\ntrainn_df, validd_df = train_test_split(train_valid_df, test_size=valid_ratio / (train_ratio + valid_ratio), random_state=42, stratify=train_valid_df[\"target_relabeled\"])\n\ntrain_df = create_dataset(trainn_df)\nvalid_df = create_dataset(validd_df)\ntest_df = create_dataset(testt_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for feat, target in train_df.take(1):\n    print(feat[0])\n    print(target[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"txt_vec = TextVectorization(max_tokens=1000, split='whitespace') # most frequent 1000 words (out of 20K uniques)\n# to prevent overfitting by many parameters\ntxt_vec.adapt(df_train['text_cleaned'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = txt_vec.vocabulary_size()\nprint(txt_vec.get_vocabulary()[:50]) # pad - UNK - words","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ = Input(shape=[], dtype=tf.string)\n\ntoken_ids = txt_vec(input_) # Text Vectorization\nembedding_vector = Embedding(input_dim=vocab_size, output_dim=64, mask_zero=True)(token_ids) # Embedding\n\nhidden = GRU(16, dropout=0.3)(embedding_vector) # GRU\n\noutput = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')(hidden)\n\ntwitter_disaster_clf = Model(inputs=[input_], outputs=[output])\ntwitter_disaster_clf.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compile & fit\noptimizer = Nadam(learning_rate=0.001)\ntwitter_disaster_clf.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\ntwitter_disaster_clf.fit(train_df, validation_data=valid_df, epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### pretrained embedding and finetune\n\n* our model is overfitting but thats bcz alot of parameters to learn so let's use pretrained embedding and finetune\n\n* Embedding matrix need a large dataset to generalize well so let's use pretrained GloVe and Universal Sentence Encoder","metadata":{}},{"cell_type":"code","source":"type(glove_embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab = txt_vec.get_vocabulary()\nembedding_dim = 300  # GloVe 840B has 300D embeddings\n\n# Initialize embedding matrix with zeros\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n# Fill the embedding matrix with GloVe vectors\nfor i, word in enumerate(vocab):\n    if word in glove_embeddings:  # Check if word exists in GloVe\n        embedding_matrix[i] = glove_embeddings[word]\n\n\nembedding_matrix.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 300 \nembedding_layer = Embedding(\n    input_dim=vocab_size,\n    output_dim=embedding_dim,\n    weights=[embedding_matrix],  # Load GloVe vectors\n    trainable=False, # no fine tune now let's try\n    mask_zero=True\n)\n\ninput_ = Input(shape=[], dtype=tf.string)\n\ntoken_ids = txt_vec(input_) # Text Vectorization\nembedding_vector = embedding_layer(token_ids) # Embedding\n\nhidden = GRU(16, dropout=0.4)(embedding_vector) # GRU\n\noutput = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')(hidden)\n\ntwitter_disaster_clf2 = Model(inputs=[input_], outputs=[output])\ntwitter_disaster_clf2.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compile & fit\noptimizer = Nadam(learning_rate=0.001)\ntwitter_disaster_clf2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\ntwitter_disaster_clf2.fit(train_df, validation_data=valid_df, epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"twitter_disaster_clf2.evaluate(test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### FineTune GloVe\n\n* with small learning rate","metadata":{}},{"cell_type":"code","source":"embedding_dim = 300 \nembedding_layer = Embedding(\n    input_dim=vocab_size,\n    output_dim=embedding_dim,\n    weights=[embedding_matrix],  # Load GloVe vectors\n    trainable=True, # no fine tune now let's try\n    mask_zero=True\n)\n\ninput_ = Input(shape=[], dtype=tf.string)\n\ntoken_ids = txt_vec(input_) # Text Vectorization\nembedding_vector = embedding_layer(token_ids) # Embedding\n\nhidden = GRU(16, dropout=0.4)(embedding_vector) # GRU\n\noutput = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')(hidden)\n\ntwitter_disaster_clf3 = Model(inputs=[input_], outputs=[output])\n\n# compile & fit\noptimizer = Nadam(learning_rate=0.0001)\ntwitter_disaster_clf3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\ntwitter_disaster_clf3.fit(train_df, validation_data=valid_df, epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"twitter_disaster_clf3.evaluate(test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Let's try stronger Encoder\n\n* 1) Universal Sentence Encoder\n\n* 2) BERT","metadata":{"execution":{"iopub.status.busy":"2025-03-03T22:34:18.635215Z","iopub.execute_input":"2025-03-03T22:34:18.635581Z","iopub.status.idle":"2025-03-03T22:34:18.715685Z","shell.execute_reply.started":"2025-03-03T22:34:18.635558Z","shell.execute_reply":"2025-03-03T22:34:18.714863Z"}}},{"cell_type":"code","source":"# 1) Universal Sentence Encoder\n\nuniversal_encoder = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\")\nuse_embed_samples = universal_encoder([\"When you use the universal encoder on a sentence, it turns sentence into numbers.\"])\nuse_embed_samples[0][:50]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# url = 'https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2'\nurl = 'https://tfhub.dev/google/universal-sentence-encoder/4'\nuniversal_encoder = hub.KerasLayer(url,\n    input_shape=[],\n    dtype=tf.string,\n    trainable=False,\n    name=\"USE\")\n\nclass UniversalEncoderLayer(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return universal_encoder(inputs)\n\n\ninput_ = Input(shape=[], dtype=tf.string)\n\nembedding_vector = UniversalEncoderLayer()(input_) # Encoder\n\nhidden = Dense(128, activation='relu', kernel_initializer='he_normal')(embedding_vector)\nhidden = BatchNormalization()(hidden)  # Normalize activations\nhidden = Dropout(0.5)(hidden)  # Reduce overfitting\nhidden = Dense(64, activation='relu', kernel_initializer='he_normal')(hidden)\nhidden = BatchNormalization()(hidden) \nhidden = Dropout(0.3)(hidden)\n\noutput = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')(hidden)\n\ntwitter_disaster_clf4 = Model(inputs=[input_], outputs=[output])\ntwitter_disaster_clf4.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compile & fit\noptimizer = Nadam(learning_rate=0.001)\ntwitter_disaster_clf4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'], jit_compile=False)\n\nearly_cb = EarlyStopping(patience=10, monitor='val_loss')\ntwitter_disaster_clf4.fit(train_df, validation_data=valid_df, epochs=100, callbacks=[early_cb])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"twitter_disaster_clf4.evaluate(test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fine tune\n# url = 'https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2'\nurl = 'https://tfhub.dev/google/universal-sentence-encoder/4'\nuniversal_encoder = hub.KerasLayer(url,\n    input_shape=[],\n    dtype=tf.string,\n    trainable=True,\n    name=\"USE\")\n\nclass UniversalEncoderLayer(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return universal_encoder(inputs)\n\n\ninput_ = Input(shape=[], dtype=tf.string)\n\nembedding_vector = UniversalEncoderLayer()(input_) # Encoder\n\nhidden = Dense(128, activation='relu', kernel_initializer='he_normal')(embedding_vector)\nhidden = BatchNormalization()(hidden)  # Normalize activations\nhidden = Dropout(0.5)(hidden)  # Reduce overfitting\nhidden = Dense(64, activation='relu', kernel_initializer='he_normal')(hidden)\nhidden = BatchNormalization()(hidden) \nhidden = Dropout(0.3)(hidden)\n\noutput = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')(hidden)\n\ntwitter_disaster_clf5 = Model(inputs=[input_], outputs=[output])\ntwitter_disaster_clf5.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compile & fit\noptimizer = Nadam(learning_rate=0.0001)\ntwitter_disaster_clf5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'], jit_compile=False)\n\nearly_cb = EarlyStopping(patience=10, monitor='val_loss')\ntwitter_disaster_clf5.fit(train_df, validation_data=valid_df, epochs=100, callbacks=[early_cb])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"twitter_disaster_clf5.evaluate(test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage, train_vocab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BERT","metadata":{}},{"cell_type":"code","source":"# BERT using Hugging Face & pytorch (finetunning)\n# get tokenizer\n\nfrom transformers import AutoTokenizer, AutoModel # tokenizer and model\n\n# Load BERT tokenizer (uncased version)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize the tweets\nencodings = tokenizer(['Hello Everyone'], padding=True, return_tensors=\"pt\") # return result as pytorch tensor\nencodings # 101 is <SOS> and 102 is <EOS>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prepare dataset\nimport torch\n\nclass TwitterDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe):\n        self.texts = dataframe[\"text_cleaned\"].tolist()\n        self.labels = torch.tensor(dataframe[\"target_relabeled\"].tolist(), dtype=torch.long)  # Convert labels to tensor\n\n        # Tokenize all text samples\n        self.encodings = tokenizer(self.texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\n\n# Create dataset\nbatch_size = 32\ntrain_dataset = TwitterDataset(trainn_df)\nvalid_dataset = TwitterDataset(validd_df)\ntest_dataset = TwitterDataset(testt_df)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model arch\n\nfrom transformers import AutoModel\nimport torch.nn as nn\n\n\nclass BertClassifier(nn.Module):\n    def __init__(self):\n        super(BertClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n        # Freeze BERT parameters\n        for param in self.bert.parameters():\n            param.requires_grad = True  # Freezing BERT\n\n        self.dropout = nn.Dropout(0.3)\n        \n        # Additional Hidden Layer\n        self.hidden = nn.Linear(768, 256)  # Extra hidden layer\n        self.relu = nn.ReLU()  # Activation function\n        \n        # Final Output Layer\n        self.fc = nn.Linear(256, 1)  # Binary classification (1 neuron)\n\n    def forward(self, input_ids, attention_mask):\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = bert_output.pooler_output  # [batch_size, 768]\n\n        x = self.dropout(pooled_output)\n        x = self.hidden(x)\n        x = self.relu(x)\n        x = self.fc(x)  # Output logits\n        return x\n\n# Initialize model\ntwitter_disaster_bert = BertClassifier()\ntwitter_disaster_bert","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import NAdam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntwitter_disaster_bert.to(device)\n\n# Loss Function (Binary Cross-Entropy)\ncriterion = nn.BCEWithLogitsLoss()  # Since output isn't sigmoid yet\n\n# Nadam Optimizer\noptimizer = NAdam(twitter_disaster_bert.parameters(), lr=0.0001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, data_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids, attention_mask, labels = (\n                batch[\"input_ids\"].to(device),\n                batch[\"attention_mask\"].to(device),\n                batch[\"labels\"].float().to(device),\n            )\n            \n            outputs = model(input_ids, attention_mask).squeeze()\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            predictions = (torch.sigmoid(outputs) > 0.5).long()\n            correct += (predictions == labels.long()).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total\n    print(f\"Validation Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=3):\n    model.train()\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch in train_loader:\n            input_ids, attention_mask, labels = (\n                batch[\"input_ids\"].to(device),\n                batch[\"attention_mask\"].to(device),\n                batch[\"labels\"].float().to(device),  # BCE requires float labels\n            )\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask).squeeze()  # Remove extra dim\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            predictions = (torch.sigmoid(outputs) > 0.5).long()\n            correct += (predictions == labels.long()).sum().item()\n            total += labels.size(0)\n        \n        train_acc = correct / total\n        print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Accuracy={train_acc:.4f}\")\n\n        # Validate after each epoch\n        evaluate_model(model, valid_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(twitter_disaster_bert, train_loader, valid_loader, criterion, optimizer, epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(model, test_loader):\n    model.eval()\n    predictions = []\n    true_labels = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids, attention_mask, labels = (\n                batch[\"input_ids\"].to(device),\n                batch[\"attention_mask\"].to(device),\n                batch[\"labels\"].to(device),  # Extract true labels\n            )\n\n            outputs = model(input_ids, attention_mask).squeeze()\n            probs = torch.sigmoid(outputs)\n            preds = (probs >= 0.5).long().cpu().numpy()\n            \n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())  # Convert true labels to numpy\n\n    return predictions, true_labels\n\n# Get predictions and true labels\ntest_predictions, y_test = predict(twitter_disaster_bert, test_loader)\nprint(test_predictions[:10])  # Print first 10 predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Assuming you have ground truth labels (y_test) for test data\naccuracy = accuracy_score(y_test, test_predictions)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Full Comparison using validation\n\n**before_finetunning -> after finetunning**\n\n**Valid**\n* 1) My Embedding - Valid: 77.03 (Overfit)\n* 2) GloVe - Valid: 80.31 -> 79.27\n* 3) USE - Valid: 82.15 -> 84.38\n* 4) BERT - Valid: 81.10\n\n**Test**\n* 2) GloVe - Test: 76.11 -> 76.64\n* 3) USE - Test: 79.39 -> 81.62\n* 4) BERT - Test: 78.22","metadata":{}},{"cell_type":"code","source":"# highest on validation is twitter_disaster_clf5\ny_test_prob = twitter_disaster_clf5.predict(df_test['text_cleaned'])\ny_test_pred = (y_test_prob >= 0.5)\ny_test_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_submission = df_test[['id']].copy()\ndf_test_submission['target'] = y_test_pred\ndf_test_submission['target'] = df_test_submission['target'].astype(int)\ndf_test_submission.to_csv('twitter_disaster_clf.csv', index=False)\ndf_test_submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"twitter_disaster_clf5.save(\"twitter_disaster_clf5.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stop","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Tensorflow BERT","metadata":{}},{"cell_type":"code","source":"# Load tokenizer and model\nfrom transformers import AutoTokenizer, TFAutoModel\n\nmodel_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntwitter_disaster_bert = TFAutoModel.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_ids = tokenizer(['Cat Of Nine Irons XII: This nightmarishly brutal weapon is used in ritualistic country club de'],\n          padding=True,\n         return_tensors=\"tf\")\n\ntoken_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prepare model for 2 outputs only\n\ntwitter_disaster_bert.trainable = False # freeze the 134M parameter wow\n\n# arch of bertweet\ninput_ids = Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\") # both come from tokenizer\n\n# Pass through BERT\nbert_output = twitter_disaster_bert(input_ids, attention_mask=attention_mask)\nhidden_state = bert_output.last_hidden_state  # (batch_size, seq_len, 768)\npooled_output = bert_output.pooler_output  # (batch_size, 768)\n\n# Dense Layers\ndropout = Dropout(0.3)(pooled_output)\nhidden = Dense(256, activation=\"relu\", keras_initilizer='he_normal')(dropout)\noutput = Dense(1, activation=\"sigmoid\", keras_initilizer='glorot_normal')(hidden)\n\n# Create Model\nbert_tweet_clf = Model(inputs=[input_ids, attention_mask], outputs=output)\nbert_tweet_clf.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) prepare X_train & y_train\ndef tokenize_data(texts, labels, shuffle=False):\n    encodings = tokenizer(texts.tolist(), padding=True)\n    \n    ds = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n    if shuffle:\n        ds.shuffle(10_000, seed=42)\n    return ds.batch(32).prefetch(1)\n\ntrain_dataset = tokenize_data(trainn_df[\"text_cleaned\"], trainn_df[\"target_relabeled\"], shuffle=True)\nvalid_dataset = tokenize_data(validd_df[\"text_cleaned\"], validd_df[\"target_relabeled\"])\ntest_dataset = tokenize_data(testt_df[\"text_cleaned\"], testt_df[\"target_relabeled\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2) Understand model output (it's logit so use Loss(from_lgits=True))\n# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = Nadam(learning_rate=0.0001)\nbert_tweet_clf.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n\n# 3) model.compile & .fit with small learning_rate & few epochs\nhistory = bert_tweet_clf.fit(train_dataset, validation_data=valid_dataset, epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stop","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}